{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1D3rYU8Gurfy5HxgFK1Jbs8_RK6UujSt5",
      "authorship_tag": "ABX9TyNAIrhiDKYqOb0rDFeFssfG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smokingsnakes83/Truth_model/blob/main/Truth_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "import textwrap\n",
        "\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "from IPython.display import Markdown\n",
        "from IPython.display import display\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "mwXEi5wq5Byu"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtém a chave de API a partir dos dados do usuário\n",
        "API_KEY=userdata.get('API_KEY')\n",
        "\n",
        "# Configura a biblioteca genai com a chave de API obtida\n",
        "genai.configure(api_key=API_KEY) #Substitua pela sua API_KEY"
      ],
      "metadata": {
        "id": "mCylqZwxJGTI"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carregando o arquivo CSV em um dataframe pandas"
      ],
      "metadata": {
        "id": "tYR9Azmkudzs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "MuQ6u0QX18Te"
      },
      "outputs": [],
      "source": [
        "# Carrega o arquivo CSV 'informacoes_ia.csv' em um dataframe pandas\n",
        "df = pd.read_csv('/content/content_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(df)\n",
        "df.columns = ['Titulo', 'Conteudo']\n",
        "#df"
      ],
      "metadata": {
        "id": "j-1DoKKP1-4H"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model**"
      ],
      "metadata": {
        "id": "J7cSW9FyvT82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo o modelo na variável model\n",
        "embed_model = 'models/embedding-001'"
      ],
      "metadata": {
        "id": "ngyF8IHuMvRT"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Função que Gera Embeddings para os Documentos**\n",
        "#**Documentação Detalhada**\n",
        "###**Função embed_fn:**<br>\n",
        "A função **embed_fn** recebe o título e o texto de um documento como entrada.\n",
        "Ela usa a função genai.embed_content (assumindo que genai é uma biblioteca ou API de embedding) para gerar um vetor de embedding que representa o documento.\n",
        "\n",
        "O argumento **model** especifica o modelo de embedding a ser usado.\n",
        "\n",
        "O argumento **task_type='retrieval_document'** indica que o embedding será usado para tarefas de recuperação de documentos.\n",
        "A função retorna o vetor de embedding do documento.\n",
        "\n",
        "###**Aplicação da função ao dataframe:**<br>\n",
        "A linha **df['Embeddings'] = df.apply(lambda row: embed_fn(row['Titulo'], row['Conteudo']), axis=1)** aplica a função embed_fn a cada linha do dataframe df.<br><br>\n",
        "**df.apply(..., axis=1)** aplica a função a cada linha (eixo 1).<br><br>\n",
        "A função **lambda lambda row: embed_fn(row['Titulo'], row['Conteudo'])** extrai o título **(row['Titulo'])** e o conteúdo **(row['Conteudo'])** de cada linha e os passa para a função **embed_fn**.<br><br>\n",
        "O resultado da função **embed_fn (o vetor de embedding)** é armazenado na nova coluna **'Embeddings'** do dataframe.\n",
        "###**Finalidade:**\n",
        "Este código gera embeddings para cada documento (representado por título e conteúdo) no dataframe, armazenando os embeddings na coluna 'Embeddings'. Isso permite que os documentos sejam comparados semanticamente usando seus embeddings, por exemplo, para realizar buscas por similaridade ou agrupamento."
      ],
      "metadata": {
        "id": "ZXNMCOKuzPEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_fn(title, text):\n",
        "\n",
        "  \"\"\"\n",
        "  Gera embeddings para documentos usando o modelo especificado.\n",
        "\n",
        "  Args:\n",
        "      title: O título do documento.\n",
        "      text: O corpo do texto do documento.\n",
        "\n",
        "  Returns:\n",
        "      Um vetor de embedding representando o documento.\n",
        "  \"\"\"\n",
        "\n",
        "  return genai.embed_content(model=embed_model,\n",
        "                             content=text,\n",
        "                             task_type='retrieval_document',\n",
        "                             title=title)['embedding']\n",
        "\n",
        "# Aplica a função embed_fn a cada linha do dataframe\n",
        "df['Embeddings'] = df.apply(lambda row: embed_fn(row['Titulo'], row['Conteudo']), axis=1)\n",
        "\n",
        "# Exibe o dataframe com a nova coluna 'Embeddings'\n",
        "#df"
      ],
      "metadata": {
        "id": "DbrrA_92LlLW"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Função que gera embeddings das consultas**\n",
        "##**Documentação detalhada:**\n",
        "###**Objetivo:**<br>\n",
        "A função ***generate_query*** visa recuperar a informação mais relevante de um conjunto de dados estruturado com base na similaridade semântica com uma consulta fornecida pelo usuário.\n",
        "###**Funcionamento:**<br>\n",
        "1. **generate_query:** A função gera um embedding da consulta usando a função *genai.embed_content*.<br>\n",
        "2. **similarity**: Em seguida, a função calcula a similaridade do cosseno entre o embedding da consulta e os embeddings armazenados no dataframe base.\n",
        "3. **Identificação da informação mais semelhante:** O índice da informação com maior similaridade é identificado.\n",
        "Verificação da similaridade: A similaridade é comparada com o limite definido.<br>\n",
        "4. **Retorno da informação:** Se a similaridade for maior ou igual ao limite, a função retorna a informação correspondente do dataframe.\n",
        "5. **Mensagem de erro:** Se a similaridade for menor que o limite, a função retorna uma mensagem informando que não foi possível encontrar uma resposta adequada.\n",
        "#**Utilização:**\n",
        "A função recebe a consulta do usuário (*query*), o dataframe contendo os embeddings e as informações (*base*), o modelo de embedding (*model*), e o limite de similaridade (*limit*).<br>\n",
        "A função retorna uma tupla contendo a informação correspondente ou a mensagem de erro, juntamente com o valor da similaridade do cosseno.\n",
        "###**Observações:**\n",
        "O código assume que a biblioteca genai está disponível e configurada.\n",
        "O dataframe base precisa conter as colunas 'Embeddings' e 'Conteudo'.\n",
        "O limite de similaridade (*limit*) pode ser ajustado para controlar a sensibilidade da busca.\n",
        "\n"
      ],
      "metadata": {
        "id": "812b5z37vvS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_query(query, base, model, limit=0.66):\n",
        "  \"\"\"\n",
        "  Gera uma consulta semântica e retorna a informação correspondente no dataframe.\n",
        "\n",
        "  Args:\n",
        "      query: A consulta do usuário.\n",
        "      base: O dataframe contendo os embeddings e as informações.\n",
        "      model: O modelo de embedding.\n",
        "      limite: Limite de similaridade para considerar uma resposta válida.\n",
        "\n",
        "  Returns:\n",
        "      A informação correspondente à consulta ou uma mensagem de erro caso a similaridade seja insuficiente.\n",
        "  \"\"\"\n",
        "\n",
        "  query_embed = genai.embed_content(model=model,\n",
        "                                    content=query,\n",
        "                                    task_type='retrieval_query') ['embedding']\n",
        "\n",
        "  dot_products = np.dot(np.stack(df['Embeddings']), query_embed)\n",
        "  idx = np.argmax(dot_products)\n",
        "\n",
        "  # Calcula a similaridade do cosseno\n",
        "  similarity = dot_products[idx] / (np.linalg.norm(query_embed) * np.linalg.norm(base['Embeddings'][idx]))\n",
        "\n",
        "  #Verificar se a maior similaridade está acima do limite\n",
        "  if similarity >= limit:\n",
        "    print('\\nSimilaridade:', similarity)\n",
        "    return df.iloc[idx]['Conteudo']\n",
        "  else:\n",
        "    print('\\nSimilaridade:', similarity)\n",
        "    return 'Sinto muito em não poder ajuda-lo, ainda não fui treinado para dar esta resposta.'"
      ],
      "metadata": {
        "id": "r0gg_s87QnIl"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configurações do modelo**"
      ],
      "metadata": {
        "id": "VE7H1umAaa8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_config = {\n",
        "    'candidate_count': 1,\n",
        "    'temperature': 0.5\n",
        "}\n",
        "\n",
        "safety_config = {\n",
        "    'harassment': 'block_none',\n",
        "    'hate': 'block_none',\n",
        "    'sexual': 'block_none',\n",
        "    'dangerous': 'block_none'\n",
        "}"
      ],
      "metadata": {
        "id": "ZrOllhQZZ6r1"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Consulta**"
      ],
      "metadata": {
        "id": "UEuDm3e-wL2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Para sair digite (q)')\n",
        "query = input('Faça sua consulta: ')\n",
        "\n",
        "while query != 'q':\n",
        "  passage = generate_query(query, df, embed_model)\n",
        "  #Markdown(f'###{passage}')\n",
        "\n",
        "  prompt = f'Reescreva este texto, aja como um especialista no assunto, mas sem adicionar informações que não faça parte do texto: {passage}'\n",
        "\n",
        "  gen_model = genai.GenerativeModel('gemini-1.5-pro-latest',\n",
        "                                          generation_config=gen_config,\n",
        "                                          safety_settings=safety_config)\n",
        "\n",
        "  response = gen_model.generate_content(prompt, stream=True)\n",
        "  response.resolve()\n",
        "\n",
        "  print()\n",
        "\n",
        "  display(to_markdown(response.text))\n",
        "  %time\n",
        "\n",
        "  print('\\nPara sair digite (q)')\n",
        "  query = input('Faça sua consulta: ') # substitua pela sua consulta"
      ],
      "metadata": {
        "id": "NBJoIQTTUUiG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}